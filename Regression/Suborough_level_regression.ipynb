{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "7KWhqyOsyFUD",
    "outputId": "92b2a0c6-cd49-4cd7-8e56-3c24b7568833"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-6m2fbR0NZa"
   },
   "outputs": [],
   "source": [
    "!chmod +x /content/drive/My\\ Drive/spark-2.3.4-bin-hadoop2.7/bin/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goYx_98Mtivn"
   },
   "outputs": [],
   "source": [
    "!pip install -q findspark\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbau0uVvsb_r"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/drive/My Drive/spark-2.3.4-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "id": "FaGe6vQxZFKs",
    "outputId": "d976bdf9-a0c3-4e0b-b861-910be7b7bb48"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "colab_type": "code",
    "id": "KdsNRhPOqNA4",
    "outputId": "9a80f8de-ffc0-4b61-999e-ade88e399157"
   },
   "outputs": [],
   "source": [
    "!pip install geospark\n",
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h36yJk65ytLm"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.jars\", \"/content/drive/My\\ Drive/geospark-sql_2.3-1.3.1.jar,/content/drive/My\\ Drive/geospark-1.3.1.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6oILq-6IiiI"
   },
   "outputs": [],
   "source": [
    "final_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"your_data\").limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8CvpDgcKX64p"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StructField, StructType\n",
    "from geospark.sql.types import GeometryType\n",
    "import geopandas as gpd\n",
    "from pyspark.sql import SparkSession\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "\n",
    "\n",
    "points = gpd.read_file(\"/content/drive/My Drive/sde-columbia-census_2000_032807211977000-shapefile.zip (Unzipped Files)/columbia_census_2000_032807211977000.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "-Xb0wb1adPmE",
    "outputId": "215641c6-b857-430e-8c17-4374e952685d"
   },
   "outputs": [],
   "source": [
    "points.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaSIVp6LcGNj"
   },
   "outputs": [],
   "source": [
    "points_geom = spark.createDataFrame(\n",
    "    points[[\"bor_subb\",\"name\",\"geometry\"]].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "FT3vrWErcvSt",
    "outputId": "8b182072-1e74-4117-de73-c8b801a8001c"
   },
   "outputs": [],
   "source": [
    "points_geom.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "LqyB_w5LnzLo",
    "outputId": "801800d8-bfe7-499a-d194-33dcd5139ecd"
   },
   "outputs": [],
   "source": [
    "points_geom.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "0gcNJiBPkCbd",
    "outputId": "9585ab06-77f1-4ffc-b4ad-55bd1e1c44ef"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "complaints_df = final_df.filter(final_df.RECORD_TYPE == 'C')\n",
    "complaints_df = complaints_df.filter(complaints_df.Lat_Lon.isNotNull())\n",
    "complaints_df = complaints_df.withColumn(\"LatLon\",f.concat(lit(\"POINT(\"), f.col(\"Longitude\"),lit(\" \"),f.col(\"Latitude\"), lit(\")\")))\n",
    "complaints_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "E_e23aHQr2lN",
    "outputId": "9f3b738d-550b-4c38-89d1-db3764a1018e"
   },
   "outputs": [],
   "source": [
    "complaints_df.createOrReplaceTempView(\"county\")\n",
    "complaints_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "B6qVo3vMJehv",
    "outputId": "a9c0a21a-781b-479f-e718-4b71d7309952"
   },
   "outputs": [],
   "source": [
    "points_geom.createOrReplaceTempView(\"pts\")\n",
    "points_geom.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "-Z7BhHPjiBNJ",
    "outputId": "4d09ea83-2353-4fae-b4a5-600892f4d878"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from geospark.register import upload_jars\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "upload_jars()\n",
    "GeoSparkRegistrator.registerAll(spark)\n",
    "counties_geom = spark.sql(\n",
    "    \"SELECT *, st_geomFromWKT(LatLon) as point from county\"\n",
    ")\n",
    "pts = spark.sql(\n",
    "    \"SELECT *,st_geomFromWKT(geometry) as shape from pts \"\n",
    ")\n",
    "counties_geom.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "EZi2OS_1JPgm",
    "outputId": "bd39e08e-9cc4-478c-b4f6-5c3fbb2ec760"
   },
   "outputs": [],
   "source": [
    "pts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGN-eFdJDekz"
   },
   "outputs": [],
   "source": [
    "counties_geom.createOrReplaceTempView(\"counties\")\n",
    "pts.createOrReplaceTempView(\"pois\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3RoFGhPIMlC-"
   },
   "outputs": [],
   "source": [
    "output = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT c.CMPLNT_FR_DT,c.CMPLNT_FR_TM,c.ADDR_PCT_CD,c.Latitude,c.Longitude, p.bor_subb,p.name\n",
    "        FROM pois AS p, counties AS c\n",
    "        WHERE ST_Intersects(p.shape, c.point)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "KSIFKm-2sQSz",
    "outputId": "5104ef66-be01-47f6-8040-4832bc3a4b72"
   },
   "outputs": [],
   "source": [
    "merged_df = spark.read.format(\"csv\").option(\"header\", \"true\").load('/content/drive/My Drive/sub.csv')\n",
    "merged_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LgQZXAPYumVP"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import concat\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6d7bs8kiW-Go"
   },
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "arrests_histroic = spark.read.json(\"hdfs://ahalya/raw_data/Arrests_2019.json\")\n",
    "arrests_2019 = spark.read.json(\"hdfs://ahalya/raw_data/Arrests_2019.json\")\n",
    "complaints_histroic = spark.read.json(\"hdfs://ahalya/raw_data/Complaints_historic.json\")\n",
    "complaints_2019 = spark.read.json(\"hdfs://ahalya/raw_data/Complaints_2019.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6BFFb9gXgpO"
   },
   "outputs": [],
   "source": [
    "# Preprocessing Utility Functions\n",
    "\n",
    "boro_dict = {\n",
    "    'B' : 'BRONX',\n",
    "    'S' : 'STATEN ISLAND',\n",
    "    'K' : 'BROOKLYN',\n",
    "    'M' : 'MANHATTAN',\n",
    "    'Q' : 'QUEENS'\n",
    "}\n",
    "\n",
    "def convert_borough_name(row):\n",
    "    row_dict = row.asDict()\n",
    "    if(row_dict['BORO_NM'] in boro_dict):\n",
    "        row_dict['BORO_NM'] = boro_dict[row_dict['BORO_NM']]\n",
    "    return Row(**row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jt-zitr_Yw72"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Merging to make arrests , complaints RDD\n",
    "arrests = arrests_histroic.union(arrests_2019)\n",
    "complaints = complaints_histroic.union(complaints_2019)\n",
    "\n",
    "# Dropping unnecessary tables\n",
    "arrests = arrests.drop('PD_CD','KY_CD','LAW_CODE', \\\n",
    "                        'LAW_CAT_CD','JURISDICTION_CODE','X_COORD_CD', \\\n",
    "                        'Y_COORD_CD')\n",
    "complaints = complaints.drop('CMPLNT_TO_DT','CMPLNT_TO_TM', \\\n",
    "                               'RPT_DT', 'KY_CD', 'PD_CD', 'LOC_OF_OCCUR_DESC', \\\n",
    "                               'JURIS_DESC','JURISDICTION_CODE','PARKS_NM','HADEVELOPT', \\\n",
    "                               'HOUSING_PSA','X_COORD_CD','Y_COORD_CD','TRANSIT_DISTRICT', \\\n",
    "                               'PATROL_BORO','STATION_NAME')\n",
    "\n",
    "# Renaming the arrest columns\n",
    "arrests = arrests.withColumnRenamed(\"ARREST_BORO\", \"BORO_NM\") \\\n",
    "                    .withColumnRenamed(\"ARREST_PRECINCT\", \"ADDR_PCT_CD\") \\\n",
    "                    .withColumnRenamed(\"AGE_GROUP\", \"SUSP_AGE_GROUP\") \\\n",
    "                    .withColumnRenamed(\"PERP_RACE\", \"SUSP_RACE\") \\\n",
    "                    .withColumnRenamed(\"PERP_SEX\", \"SUSP_SEX\")\\\n",
    "                    .withColumnRenamed(\"ARREST_KEY\", \"CMPLNT_NUM\")\n",
    "                \n",
    "# Convert borough names for arrests\n",
    "arrests = arrests.rdd.map(lambda row : convert_borough_name(row)).toDF()\n",
    "\n",
    "# Filtering age group\n",
    "arrests = arrests.filter(arrests.SUSP_AGE_GROUP.isin(['<18', '18-24' , '25-44', '45-64', '65+']))\n",
    "complaints = complaints.filter(complaints.SUSP_AGE_GROUP.isin(['<18', '18-24' , '25-44', '45-64', '65+', 'UNKNOWN']) | complaints.SUSP_AGE_GROUP.isNull())\n",
    "\n",
    "# Filtering sex\n",
    "complaints = complaints.filter(complaints.SUSP_SEX.isin(['M', 'F', 'U']) | complaints.SUSP_SEX.isNull())\n",
    "complaints = complaints.filter(complaints.VIC_SEX.isin(['M', 'F', 'U', 'D', 'E']) | complaints.VIC_SEX.isNull())\n",
    "\n",
    "\n",
    "# Coverting the columns to suitable datatype\n",
    "complaints = complaints.withColumn(\"ARREST_DATE\", lit(None).cast(StringType())) \\\n",
    ".withColumn(\"RECORD_TYPE\", lit('C'))\\\n",
    ".withColumn('Year', fn.year(fn.to_timestamp('CMPLNT_FR_DT', 'MM/dd/yyyy')))\n",
    "\n",
    "arrests = arrests.withColumn(\"CMPLNT_FR_DT\", lit(None).cast(StringType())) \\\n",
    ".withColumn(\"CMPLNT_FR_TM\", lit(None).cast(StringType())) \\\n",
    ".withColumn(\"OFNS_DESC\", lit(None).cast(StringType())) \\\n",
    ".withColumn(\"CRM_ATPT_CPTD_CD\", lit(None).cast(StringType())) \\\n",
    ".withColumn(\"LAW_CAT_CD\", lit(None).cast(StringType())) \\\n",
    ".withColumn(\"PREM_TYP_DESC\", lit(None).cast(StringType())) \\\n",
    ".withColumn(\"Lat_Lon\", lit(None).cast(StringType())) \\\n",
    ".withColumn(\"VIC_AGE_GROUP\", lit(None).cast(StringType())) \\\n",
    ".withColumn(\"VIC_RACE\", lit(None).cast(StringType())) \\\n",
    ".withColumn(\"VIC_SEX\", lit(None).cast(StringType())) \\\n",
    ".withColumn(\"RECORD_TYPE\", lit('A'))\\\n",
    ".withColumn('Year', fn.year(fn.to_timestamp('CMPLNT_FR_DT', 'MM/dd/yyyy')))\n",
    "\n",
    "# Dataset\n",
    "crime_data = arrests.union(complaints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "coA9KHx8aabQ"
   },
   "outputs": [],
   "source": [
    "# Preprocessing demograhics data\n",
    "keep_column = ['2006', '2007', '2008', '2009', '2010',\n",
    "       '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "\n",
    "poverty_rate_df = pd.read_csv(\"hdfs://ahalya/raw_data/sub-borougharea-povertyrate.csv\").drop(columns=['short_name','long_name'])\n",
    "poverty_rate_df.rename(columns={'Sub-Borough Area':'Borough'}, inplace=True)\n",
    "poverty_rate = pd.melt(poverty_rate_df, id_vars=['Borough'], value_vars = keep_column)\n",
    "poverty_rate = poverty_rate.rename(columns={'variable':'Year', 'value': 'proverty_rate'})\n",
    "\n",
    "income_div_df = pd.read_csv(\"hdfs://ahalya/raw_data/sub-borougharea-incomediversityratio.csv\").drop(columns=['short_name','long_name'])\n",
    "income_div_df.rename(columns={'Sub-Borough Area':'Borough'}, inplace=True)\n",
    "income_div = pd.melt(income_div_df, id_vars=['Borough'], value_vars = keep_column)\n",
    "income_div = income_div.rename(columns={'variable':'Year', 'value': 'income_diversity'})\n",
    "\n",
    "racial_div_df = pd.read_csv(\"hdfs://ahalya/raw_data/sub-borougharea-racialdiversityindex.csv\").drop(columns=['short_name','long_name'])\n",
    "racial_div_df.rename(columns={'Sub-Borough Area':'Borough'}, inplace=True)\n",
    "racial_div = pd.melt(racial_div_df, id_vars=['Borough'], value_vars = keep_column)\n",
    "racial_div = racial_div.rename(columns={'variable':'Year', 'value': 'racial_diversity'})\n",
    "\n",
    "unemployment_df = pd.read_csv(\"hdfs://ahalya/raw_data/sub-borougharea-unemploymentrate.csv\").drop(columns=['short_name','long_name'])\n",
    "unemployment_df.rename(columns={'Sub-Borough Area':'Borough'}, inplace=True)\n",
    "unemployment = pd.melt(unemployment_df, id_vars=['Borough'], value_vars = keep_column)\n",
    "unemployment = unemployment.rename(columns={'variable':'Year', 'value': 'unemployment_rate'})\n",
    "\n",
    "pop_df = pd.read_csv(\"hdfs://ahalya/raw_data/sub-borougharea-population.csv\").drop(columns=['short_name','long_name'])\n",
    "pop_df.rename(columns={'Sub-Borough Area':'Borough'}, inplace=True)\n",
    "pop = pd.melt(pop_df, id_vars=['Borough'], value_vars = keep_column)\n",
    "pop = pop.rename(columns={'variable':'Year', 'value': 'population'})\n",
    "\n",
    "# Joining the datasets\n",
    "merge1 = pd.merge(poverty_rate, income_div, how='inner', on=[\"Borough\", \"Year\"])\n",
    "merge2 = pd.merge(racial_div, unemployment, how='inner', on=[\"Borough\", \"Year\"])\n",
    "merge3 = pd.merge(merge2, pop, how='inner', on=[\"Borough\", \"Year\"])\n",
    "demographics_df = pd.merge(merge1, merge3, how='inner', on=[\"Borough\", \"Year\"])\n",
    "demographics_df[\"Borough\"] = demographics_df[\"Borough\"].str.upper() \n",
    "demographics_df[\"Demo_Key\"] = demographics_df[\"Borough\"]+demographics_df[\"Year\"]\n",
    "\n",
    "demographics_df.to_csv(\"hdfs://ahalya/raw_data/borough_demographics.csv\",  index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLcEaIuLFwzw"
   },
   "outputs": [],
   "source": [
    "# Preparing data for linear regression\n",
    "\n",
    "# Finding borough level aggregate\n",
    "complaints_df = complaints.filter(col(\"Year\").isin(keep_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "Cp39amjZjPAL",
    "outputId": "4beeeec9-29c4-41aa-f24f-36f68380dc04"
   },
   "outputs": [],
   "source": [
    "complaints_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BtclPU3ldRl4"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StructField, StructType\n",
    "from geospark.sql.types import GeometryType\n",
    "import geopandas as gpd\n",
    "from pyspark.sql import SparkSession\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "\n",
    "\n",
    "points = gpd.read_file(\"NYC_Shape.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwVNOZXUdRl-"
   },
   "outputs": [],
   "source": [
    "points_geom = spark.createDataFrame(\n",
    "    points[[\"bor_subb\",\"name\",\"geometry\"]].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "M_3H8D5DdoKh",
    "outputId": "048d4709-9d3a-4aab-a7e7-1d43ad4e08ad"
   },
   "outputs": [],
   "source": [
    "complaints_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcczRTavdRmD"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "complaints_df = complaints_df.filter(complaints.Lat_Lon.isNotNull())\n",
    "complaints_df = complaints_df.filter(complaints.Latitude.isNotNull())\n",
    "complaints_df = complaints_df.filter(complaints.Longitude.isNotNull())\n",
    "complaints_df = complaints_df.withColumn(\"LatLon\",f.concat(lit(\"POINT(\"), f.col(\"Longitude\"),lit(\" \"),f.col(\"Latitude\"), lit(\")\")))\n",
    "complaints = complaints_df.limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "TNow7O5sdRmF",
    "outputId": "3703d213-d58a-4d8c-d144-573b663029ed"
   },
   "outputs": [],
   "source": [
    "colsToKeep = ['CMPLNT_FR_DT','CMPLNT_FR_TM','ADDR_PCT_CD','Latitude','Longitude','Year', 'LatLon']\n",
    "complaints = complaints[colsToKeep]\n",
    "complaints = complaints.na.drop(subset = ['CMPLNT_FR_DT','CMPLNT_FR_TM','ADDR_PCT_CD','Latitude','Longitude','Year'])\n",
    "complaints.createOrReplaceTempView(\"county\")\n",
    "complaints.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "KuQ8o0lSdRmH",
    "outputId": "cd12fe88-e901-40db-ee8d-216476c4464e"
   },
   "outputs": [],
   "source": [
    "points_geom.na.drop(subset=['bor_subb','name','geometry'])\n",
    "points_geom.createOrReplaceTempView(\"pts\")\n",
    "points_geom.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pi-khLQdRmJ"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from geospark.register import upload_jars\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "upload_jars()\n",
    "GeoSparkRegistrator.registerAll(spark)\n",
    "counties_geom = spark.sql(\n",
    "    \"SELECT *, st_geomFromWKT(LatLon) as point from county\"\n",
    ")\n",
    "pts = spark.sql(\n",
    "    \"SELECT *,st_geomFromWKT(geometry) as shape from pts \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0zNLEW7NdRmN"
   },
   "outputs": [],
   "source": [
    "counties_geom.createOrReplaceTempView(\"counties\")\n",
    "pts.createOrReplaceTempView(\"pois\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "69rl1L6CdRmP",
    "outputId": "269e33ba-3969-4192-f6f6-e7f402c0e69e"
   },
   "outputs": [],
   "source": [
    "output = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT c.CMPLNT_FR_DT,c.CMPLNT_FR_TM,c.ADDR_PCT_CD,c.Latitude,c.Longitude, p.bor_subb,p.name\n",
    "        FROM pois AS p, counties AS c\n",
    "        WHERE p.shape IS NOT NULL AND c.point is not null and st_intersects(p.shape, c.point) limit 1\n",
    "    \"\"\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tw5r6Z6XiR9G"
   },
   "outputs": [],
   "source": [
    "complaints_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOlU00MTW5Or"
   },
   "outputs": [],
   "source": [
    "#Preparing data for Regression\n",
    "complaints_df = output\n",
    "complaints_df.withColumn(\"KeyNew\",fn.concat(fn.col(\"name\"),fn.col(\"Year\")))\n",
    "crime_count = complaints_df.groupby(\"KeyNew\").agg(fn.count(col('CMPLNT_NUM')).alias('crime_count'))\n",
    "\n",
    "\n",
    "# Preprocessing in borough level data\n",
    "demographics = spark.read.option(\"header\", \"true\").csv(\"hdfs://ahalya/raw_data/borough_demographics.csv\")\n",
    "demographics = demographics.withColumn(\"unemployment_rate\", demographics[\"unemployment_rate\"].cast(DoubleType()))\n",
    "demographics = demographics.withColumn(\"racial_diversity\", demographics[\"racial_diversity\"].cast(DoubleType()))\n",
    "demographics = demographics.withColumn(\"income_diversity\", demographics[\"income_diversity\"].cast(DoubleType()))\n",
    "demographics = demographics.withColumn(\"proverty_rate\", demographics[\"proverty_rate\"].cast(DoubleType()))\n",
    "demographics = demographics.withColumn(\"population\", demographics[\"population\"].cast(DoubleType()))\n",
    "\n",
    "# Combining Data\n",
    "final_data = crime_count.join(demographics, crime_count.KeyNew == demographics.Demo_Key)\n",
    "final_data = final_data.drop('Demo_Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1XHlKckFW5h-"
   },
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "for i in final_data.columns:\n",
    "    if not( isinstance(final_data.select(i).take(1)[0][0], six.string_types)):\n",
    "        print( \"Correlation to crime_count for \", i, final_data.stat.corr('crime_count',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8-miphgXa5j"
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def linear_regression_inbuilt(data, dependent_vars):\n",
    "    vectorAssembler = VectorAssembler(inputCols = dependent_vars, outputCol = 'features')\n",
    "    vector_df = vectorAssembler.transform(data)\n",
    "    vector_df = vector_df.select(['features', 'crime_count'])\n",
    "#     glr = GeneralizedLinearRegression(family=\"binomial\", link=\"logit\", maxIter=10, regParam=0.0)\n",
    "#     lr_model = glr.fit(data)\n",
    "    lr = LinearRegression(featuresCol = 'features', labelCol='crime_count', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "    lr_model = lr.fit(vector_df)\n",
    "    return lr_model\n",
    "\n",
    "def linear_regression(data, indexes):\n",
    "    x = []\n",
    "    y = []\n",
    "    for each in data:\n",
    "        x_s = [ float(each[index]) for index in indexes if each[index] != None]\n",
    "        if len(x_s) == len(indexes):\n",
    "            x.append(x_s)\n",
    "            y.append(float(each[1]))\n",
    "            \n",
    "    N = len(x)\n",
    "    M = len(indexes)\n",
    "    df = N - (1+M)\n",
    "    X = np.reshape(x,(N,M))\n",
    "    Y = np.reshape(y,(len(y),1))\n",
    "    \n",
    "    if df <= 0:\n",
    "        return -1\n",
    "    \n",
    "    if N > 1:\n",
    "        X = (X - np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "        Y = (Y - np.mean(Y))/ np.std(Y)\n",
    "    \n",
    "    X = np.hstack((X,np.ones((N,1))))\n",
    "    X_t = np.transpose(X)\n",
    "    X_inv = np.linalg.pinv(np.dot(X_t,X))\n",
    "    weights = np.dot( np.dot(X_inv,X_t) ,Y)\n",
    "    \n",
    "    rss = np.sum(np.power((Y - np.dot(X, weights)), 2))\n",
    "    s_squared = rss / df\n",
    "    se = np.sum(np.power((X[:, 0]), 2))\n",
    "    tt = (weights[0, 0] / np.sqrt(s_squared / se))\n",
    "    \n",
    "    pval = stats.t.sf(np.abs(tt), df) \n",
    "    \n",
    "    return weights[0][0],pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-PgDiTfYm87C"
   },
   "outputs": [],
   "source": [
    "# .coefficients, lr_model.intercept lr_model.summary \n",
    "                                                   \n",
    "lr_model = linear_regression_inbuilt(final_data, [\"unemployment_rate\"])\n",
    "model_summary = lr_model.summary\n",
    "print(\"r2: %f\" % model_summary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oME4DUpDZHen"
   },
   "outputs": [],
   "source": [
    "result = final_data.rdd.map(list).collect()\n",
    "\n",
    "# significance of variables with crime_rate \n",
    "print(\"crime_rate vs proverty_rate \"+str(linear_regression(result, [4])[1]))\n",
    "print(\"crime_rate vs income_diversity \"+ str(linear_regression(result, [5])[1]))\n",
    "print(\"crime_rate vs racial_diversity \"+str(linear_regression(result, [6])[1]))\n",
    "print(\"crime_rate vs unemployment_rate \"+str(linear_regression(result, [7])[1]))\n",
    "\n",
    "#  significance of variables with crime_rate controlled by population\n",
    "print(\"crime_rate vs proverty_rate controlled by population \"+str(linear_regression(result, [4,8])[1]))\n",
    "print(\"crime_rate vs income_diversity controlled by population \"+ str(linear_regression(result, [5,8])[1]))\n",
    "print(\"crime_rate vs racial_diversity controlled by population \"+str(linear_regression(result, [6,8])[1]))\n",
    "print(\"crime_rate vs unemployment_rate controlled by population  \"+str(linear_regression(result, [7,8])[1]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Suborough.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
